# 1. Project description

The goal of the project is to generate images based on vectoral data. In every image, a floating glowing ball is shown in a dark room. There are two aspects that vary in every image, this being the color and the position of the floating ball.

Every scene is represented by a vector of five floating numbers: 

- Three values representing the RGB-color that the glowing ball emits
- Two values representing the position (x and y axis) of the ball

Based on this vector (also called the image's feature), a Deep Learning model tries to re-generate an image of the original scene.

# 2. Blender

Since the number of images with a *floating glowing ball in a dark room* is sparse, another way to collect this data needs to be found. [Blender](https://www.blender.org/), a free and open source 3D creation suite, comes to the rescue! Blender is an easy to use program with a vibrant community that covers all artistic needs ranging from creating art to creating data for Machine Learning.

## 2.1. Scene

For our use-case, we'll only need a simple scene that has a room (this being a floor and two walls), as well as a floating ball. To make the scene more visually pleasing, we'll make the ball glow and the floor highly reflective (since in Blender, you can do whatever tickles your fancy).

![Blender scene](https://github.com/RubenPants/BlenderCNN/blob/main/images/blender_combined.png?raw=true)

## 2.2. Code

Luckily, you don't need to change and render each scene manually in order to collect your data. More even, you can write your own Python scripts in Blender to automate this process! The following lines of code show you the most important methods to call in order to automate the data generation. To experiment with this code yourself, open the *Scripting* tab in the Blender-scene included in the repo. **Note:** The code shown below assumes that you've already created a Blender scene. 

In order to let the Blender-magic spark, you'll need to import *blenderpy* (`bpy`) package first:

```python
import bpy
```

To focus on the right object (i.e. the object you want to augment) in your blender scene, run:

```python
so = bpy.context.scene.objects["<name-of-your-object>"]
```

You can make changes to your object as follows:

```python
# Move to random location
so.location[0] = uniform(-1, 1)  # x-axis
so.location[1] = uniform(-1, 1)  # y-axis

# Change color of the 'Glow' material
material = bpy.data.materials["Glow"]. \
	node_tree.nodes["Emission"].inputs[0]
material.default_value = get_color()  # RGBA color, e.g. (.1,.2,.3,1.)
```

At last, once you're satisfied with how you're scene currently is, you can render it ('save the image' in layman terms):

```python
bpy.context.scene.render.filepath = os.path.join(
    '<your-path>', 
    '<filename>.png',
)
bpy.ops.render.render(write_still=True)
```

**Note:** The speed at which each data sample is rendered depends on the Graphic card you have (or the lack of, God have mercy on your soul if so).

# 3. Deep learning model

The goal of our model is to generate an image based on a single feature vector of five numbers. The `Transpose Convolutional Neural Network` layers are perfectly suited for this problem!

The model, as depicted in the image below, starts out with a fully connected (`Dense`) layer, followed by four layers that each consist of a `Conv2DTranspose`, `BatchNormalization` and `ReLU` layer, with the only exception being the last layer that trades the `ReLU` for a `Sigmoid` since we want the output values to be between 0 and 1. The model is implemented in [Keras](https://keras.io/).

![Model architecture](https://github.com/RubenPants/BlenderCNN/blob/main/images/architecture.png?raw=true)

# 4. Training

For training and evaluation, 10.000 samples are used; 9.000 for training itself and 1.000 for validation. To get a clear overview of the training process, [TensorBoard](https://www.tensorflow.org/tensorboard) is used, as shown in the image below. For training, the Adam optimizer with a Mean Squared Error loss is used. 

![TensorBoard overview](https://github.com/RubenPants/BlenderCNN/blob/main/images/tensorboard.png?raw=true)

# 5. Results

The results are very satisfactory, as shown in the image below which compares the true labels (*Real images*) with those generated by our model (*Generated images*).

![Results side-by-side comparison](https://github.com/RubenPants/BlenderCNN/blob/main/images/sample_combined.png?raw=true)

# 6. Sources

- https://docs.blender.org/api/current/index.html 
- https://keras.io/api/layers/convolution_layers/convolution2d_transpose/
